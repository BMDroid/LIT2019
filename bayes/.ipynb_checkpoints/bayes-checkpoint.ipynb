{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import operator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_01(fileName):\n",
    "    with open(fileName, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lst = list(reader)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_02(fileName):\n",
    "    with open(fileName, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lst = list(reader)\n",
    "    data = []\n",
    "    labels = []\n",
    "    conclusions = []\n",
    "    for l in lst[1:2]: # if the table is complete, the slicing will not be needed\n",
    "        case = []\n",
    "        label = list(map(int, l[8].strip().replace('X', '-1').split('_')))\n",
    "        conclusion = 1 if l[4] == 'OS' else 0\n",
    "        case.extend(l[9:15])\n",
    "        data.append(case)\n",
    "        labels.append(label)\n",
    "        conclusions.append(conclusion)\n",
    "        print(conclusions)\n",
    "    return data, labels, conclusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "fileName = './Data.csv'\n",
    "lst= load_data_01(fileName)\n",
    "print(len(lst[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "data, labels, conclusions = load_data_02(fileName)\n",
    "case1 = data[0]\n",
    "case2 = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list_01(dataSet):\n",
    "    vocabSet = set()\n",
    "    for case in dataSet:\n",
    "        caseText = ' '.join(case[8::]).split(' ')\n",
    "        vocabSet = vocabSet | set(caseText)\n",
    "    return sorted(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 3, 9, 5, 6, 7]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "dataSetTest = data\n",
    "print(labels)\n",
    "vocabLst = create_vocab_list_01(dataSetTest)\n",
    "print(conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stop_words(stopFileName):\n",
    "    file = open(stopFileName)\n",
    "    stopWords = list(map(lambda x: x[0:-1], file.readlines()))\n",
    "    return stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(lst, stopWords):\n",
    "    puncs = string.punctuation + '’' + '“' + '”'\n",
    "    return list(map(lambda x: x.lower(), list(filter(lambda x:  4 < len(x) <= 20 and not any(p in x for p in puncs) and x not in stopWords, lst)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopFileName = './stopWordsNot.txt'\n",
    "stopWords = read_stop_words(stopFileName)\n",
    "cleanVocabLst = clean_text(vocabLst, stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanVocabLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list_02(dataSet):\n",
    "    vocabList = []\n",
    "    for case in dataSet:\n",
    "        caseText = ' '.join(case).split()\n",
    "        vocabList.extend(caseText)\n",
    "    cleanVocabList = clean_text(vocabList, stopWords)\n",
    "    vocabDict = dict(Counter(cleanVocabList))\n",
    "    vocabDictSorted = sorted(vocabDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return list(map(lambda x: x[0], vocabDictSorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabDict = create_vocab_list_02(dataSetTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marks', 'class', 'services', 'similarity', 'finding', 'degree', 'based', 'aural', 'competing', 'conceptual', 'common', 'saintly', 'business', 'agree', 'hotel', 'foregoing', 'substantial', 'visually', 'dissimilar', 'difficulty', 'dismiss', 'reason', 'there', 'place', 'singapore', 'relation', 'places', 'james', 'location', 'linked', 'marketing', 'management', 'inseparable', 'similar', 'restaurant', 'registration', 'question', 'relevant', 'factors', 'likelihood', 'confusion', 'disturb', 'distinctiveness', 'component', 'technical', 'strength', 'argument', 'applicant', 'opponent', 'evoke', 'although', 'regis', 'conjure', 'image', 'manifestation', 'names', 'ranging', 'schools', 'roads', 'martins', 'hospitals', 'condominiums', 'nicholas', 'assortment', 'public', 'flatted', 'power', 'station', 'saint', 'julien', 'association', 'whatsoever', 'character', 'noted', 'connotes', 'geographical', 'necessarily', 'share', 'tendency', 'connote', 'building', 'reference', 'judge', 'accepted', 'adjunct', 'primary', 'reasoning', 'extent', 'closely', 'related', 'application', 'classes', 'considered', 'compared', 'collectively', 'sought', 'registered', 'regard', 'relating', 'british', 'sugar', 'robertson', 'stage', 'consideration', 'practical', 'purposes', 'applying', 'findings', 'overlap', 'users', 'administration', 'service', 'providing', 'venues', 'conferences', 'advertising', 'relate', 'promotion', 'therefore', 'notwithstanding', 'rejection', 'doctrine', 'initial', 'interest', 'reverse', 'dismissal', 'opposition', 'affirm', 'essence', 'disagree', 'point', 'respective', 'satisfied', 'circumstances', 'sufficient', 'average', 'customer', 'confused', 'owners', 'economically', 'impermissible', 'infraction', 'rights', 'permit']\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "print(vocabDict)\n",
    "print(len(vocabDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the dict match the different sections with the index in the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDict = {'Marks Similarity': 0, 'Visual Similarity':1, 'Aural Similarity':2, 'Conceptual Similarity': 3, 'G&S Similarity': 4, 'Likelyhood of Confusion': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDict = {'Marks Similarity': 0, 'Visual Similarity':1, 'Aural Similarity':2, 'Conceptual Similarity': 3, 'G&S Similarity': 4, 'Likelyhood of Confusion': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list_03(dataSet, section):\n",
    "    vocabList = []\n",
    "    idx = similarityDict[section]\n",
    "    for case in dataSet:\n",
    "        caseText = case[idx].split()\n",
    "        vocabList.extend(caseText)\n",
    "    cleanVocabList = clean_text(vocabList, stopWords)\n",
    "    vocabDict = dict(Counter(cleanVocabList))\n",
    "    vocabDictSorted = sorted(vocabDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return list(map(lambda x: x[0], vocabDictSorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = 'Marks Similarity'\n",
    "section = 'Aural Similarity'\n",
    "section = 'Visual Similarity'\n",
    "section = 'Conceptual Similarity'\n",
    "visualVocabList = create_vocab_list_03(dataSetTest, section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marks', 'saintly', 'place', 'singapore', 'common', 'places', 'location', 'conceptual', 'similarity', 'there', 'strength', 'argument', 'applicant', 'opponent', 'evoke', 'although', 'regis', 'conjure', 'image', 'manifestation', 'names', 'relation', 'ranging', 'schools', 'roads', 'martins', 'hospitals', 'condominiums', 'nicholas', 'assortment', 'public', 'flatted', 'james', 'power', 'station', 'saint', 'julien', 'association', 'whatsoever', 'character', 'noted', 'connotes', 'geographical', 'necessarily', 'share', 'tendency', 'connote', 'building', 'competing', 'linked', 'reference']\n"
     ]
    }
   ],
   "source": [
    "print(visualVocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: edit load_data() to suit make different data or maybe not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_section_data(dataSet, labels, conclusions, section):\n",
    "    section = similarityDict[section]\n",
    "    lst = []\n",
    "    for i, case in dataSet:\n",
    "        content = case[idx].split()\n",
    "        label = str(labels[idx])\n",
    "        conclusion = str(conclusions)\n",
    "        if label != -1:\n",
    "            cleanContent = clean_text(content, stopWords)\n",
    "            cleanContent.append(label)\n",
    "            lst.append(cleanContent)\n",
    "        else:\n",
    "            continue\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 3, 9, 5, 6, 7]]\n",
      "[1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-279-8909ba11350c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconceptualData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_section_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataSetTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconclusions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconceptualData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-278-9d2af766c4d3>\u001b[0m in \u001b[0;36mcreate_section_data\u001b[0;34m(dataSet, labels, conclusions, section)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mconclusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconclusions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "conceptualData = create_section_data(dataSetTest, labels, conclusions, section)\n",
    "print(conceptualData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conceptualTest.txt', 'w') as file:\n",
    "    file.writelines(' '.join(i) + '\\n' for i in conceptualData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
